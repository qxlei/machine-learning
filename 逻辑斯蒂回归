#初始化能够用到的函数库
import matplotlib.pyplot as plt
import numpy  as np
from scipy import optimize###？？？？？？？？？？？？？？？？？？？？？
from matplotlib.font_manager import FontProperties
font = FontProperties(fname = r"c:\windows\fonts\simsun.ttc",size=14)
import os
os.chdir(r"E:\python\MachineLearning_Python-master\LogisticRegression")

#载入数据，以及数据的初始化
data = np.loadtxt("data2.txt",delimiter=',',dtype =np.float64)
X=data[:,0:-1]
y = data[:,-1]
y = y.reshape(-1,1)
#如果在一维数据上不能将数据进行划分，可以将数据映射到高维上，也就是多项式
#plot_data(X,y)
X = mapFeature(X[:,0],X[:,1])
#初始化theta值
initial_theta = np.ones((X.shape[1],1))
#正则化参数alpha
initial_lambda = 0.1
#下一步通过权重theta，数据X以及标签可以计算预测值了，这时候需要知道映射函数sigmoid
alpha =0.01
maxCycles = 1000

    
#================================================
def mapFeature(X1,X2):
    #确定映射的最高次方 
    degree =2#degree=2实际上包括degree的情况
    out =np.ones((X1.shape[0],1))#先初始化,这一列是常数项
    for i in range(1,degree+1):
        for j in range(i+1):
            temp = X1**(i-j)*X2**j
            out = np.hstack((out,temp.reshape(-1,1)))
    return out
#数据处理部分完成了，接下里应该是权重参数的初始化
#=================================================

def sigmoid(z):
    h = np.zeros((len(z),1))
    h = 1.0 / (1.0+np.exp(-z))
    return h
#预测值计算出来了，下一步开始计算误差函数
#===============================================
def costFunction(initial_theta,X,y,initial_lambda):
    #将所有预测值与真实值的误差结果累加即可
    m = len(y)
    J =0
    #需要定义映射函数sigmoid
    h = sigmoid(np.dot(X,initial_theta))
    #下一步计算误差，注意：误差包含正则化项
    J = -np.sum(y*np.log(h))-np.sum((1-y)*np.log(1-h))+initial_lambda*np.sum(initial_theta*initial_theta)/2/m
    return J
#代价函数求完了之后就应该进行权重系数的更新
#先写出梯度的计算方法：
#===============================================
def gradient(initial_theta,X,y,initial_lambda,alpha,maxCycles ):
    m = len(y)
    
    
    grad = np.zeros((initial_theta.shape[0],1))#n*1
    for i in range(maxCycles):
        theta1=initial_theta.copy()
        theta1[0]=0#第零项不在正则化项之中
        h = sigmoid(np.dot(X,initial_theta))#(m*n)*(n*1)
        grad = np.dot(np.transpose(X),h-y)/m +initial_lambda/m*theta1
        initial_theta = initial_theta - alpha*grad
    return initial_theta
#===============================================
#下一步计算如何进行梯度的更新
def predict(X,thetaFinal):
    m=X.shape[0]
    p = np.zeros((m,1))
    p=sigmoid(np.dot(X,thetaFinal))
    for i in range(m):
        if p[i]>0.5:
            p[i]=1
        else:
            p[i]=0
    return p

def LogisticRegression(X,y,initial_lambda,initial_theta,alpha,maxCycles):#循环次数未写
    J = costFunction(initial_theta,X,y,initial_lambda)
    print(J)
    
    thetaFinal = gradient(initial_theta,X,y,initial_lambda,alpha,maxCycles)
    J = costFunction(thetaFinal,X,y,initial_lambda)
    print(J)
    #计算误差
    p = predict(X,thetaFinal)
    print("在训练集上的误差为：%f"%np.mean(np.float64(p==y)*100))   
